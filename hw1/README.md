# ДЗ №1

## Эксперимент 1: Простая модель
**Архитектура:**
- Входные данные → `hidden_size` (32)
- Блок: 
  - `Linear(hidden_size → hidden_size*4)`
  - `ReLU`
  - `Linear(hidden_size*4 → hidden_size)`
- Выход: `Linear(hidden_size → 1)`

**Параметры:**
- Оптимизатор: SGD с `lr=0.01`
- Количество эпох: 50
- Размер батча: 32
- Сид: произвольный

**Результаты:**
- Оптимальное количество эпох: **9**
- После 9-й эпохи `eval_loss` начинает расти → **переобучение**

**Вывод:**  
Модель быстро переобучается. Требуется регуляризация или уменьшение сложности.

---

## Эксперимент 2: Модель побольше
**Изменения:**
- `hidden_size=128`
- 3 последовательных блока вместо одного

**Результаты:**
- Скорость обучения: **5 эпох** для достижения метрики 0.9
- После 5-й эпохи: **сильное переобучение**

**Вывод:**  
Увеличение сложности ускорило обучение, но усилило переобучение. Требуются методы регуляризации.

---

## Эксперимент 3: Skip Connections и Batch Norm
**Изменения:**
- Добавлены:
  - Skip-connection между входом и выходом блока
  - `BatchNorm` в начале каждого блока

**Результаты:**
- Скорость обучения: **3 эпохи** для метрики 0.9
- После 3-й эпохи: **сильное переобучение**
- Качество сопоставимо с Экспериментом 2

**Вывод:**  
Улучшилась сходимость, но переобучение сохранилось. Требуется дополнительная регуляризация.

---

## Эксперимент 4: Dropout
**Изменения:**
- Добавлен `Dropout` внутри блока
- Перебраны значения `p`: 0.01, 0.2, 0.9

**Результаты:**
- `p=0.01`: Быстрое переобучение
- `p=0.2`: Переобучение после 2-й эпохи, но качество растет
- `p=0.9`: Недообучение

**Вывод:**  
Оптимальное значение `p=0.2`, но требуется совместное использование с другими методами.

---

## Эксперимент 5: Weight Decay и Learning Rate
**Изменения:**
- Добавлен `weight_decay` (0.01, 0.1, 0.001)
- Перебор `lr`: 0.01, 0.05, 0.1

**Результаты:**
- Лучшая комбинация: `weight_decay=0.01` + `lr=0.01`
- При `weight_decay=0.1` качество падает
- При `lr=0.1` модель не сходится

**Вывод:**  
Оптимальные параметры: `weight_decay=0.01`, `lr=0.01`. Высокие значения ухудшают качество.

---

## Общие выводы
1. **Регуляризация критически важна** — даже при малых `hidden_size` наблюдается переобучение.
2. **Оптимальные гиперпараметры:**
   - `hidden_size=128`
   - `Dropout=0.2`
   - `weight_decay=0.01`
   - `lr=0.01`
3. **Рекомендации:**
   - Добавить раннюю остановку (early stopping)
   - Попробовать другие методы регуляризации (e.g., Label Smoothing)
   - Исследовать архитектуры с Self-Attention